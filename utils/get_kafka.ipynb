{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://913169d437dc:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Streaming dasdadsaas Kafka</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9928105540>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Spark Session\n",
    "from pyspark.sql import SparkSession # type: ignore\n",
    "\n",
    "spark = (\n",
    "    SparkSession \n",
    "    .builder \n",
    "    .appName(\"Streaming dasdadsaas Kafka\") \n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", True) \n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0')\n",
    "    .config(\"spark.sql.shuffle.partitions\", 4)\n",
    "    .master(\"local[*]\") \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kafka_df = (\n",
    "    spark\n",
    "    .read\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"ed-kafka:29092\")\n",
    "    .option(\"subscribe\", \"climate_data.temperature_data\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "root\n",
       " |-- key: binary (nullable = true)\n",
       " |-- value: binary (nullable = true)\n",
       " |-- topic: string (nullable = true)\n",
       " |-- partition: integer (nullable = true)\n",
       " |-- offset: long (nullable = true)\n",
       " |-- timestamp: timestamp (nullable = true)\n",
       " |-- timestampType: integer (nullable = true)\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kafka_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n",
       "  File \"c:\\Users\\SD-LORENZO-PC\\.vscode\\extensions\\ms-python.python-2024.22.0-win32-x64\\python_files\\python_server.py\", line 133, in exec_user_input\n",
       "    retval = callable_(user_input, user_globals)\n",
       "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"<string>\", line 6, in <module>\n",
       "  File \"C:\\Users\\SD-LORENZO-PC\\pyproject\\rndPy\\my_env\\Lib\\site-packages\\pyspark\\sql\\dataframe.py\", line 947, in show\n",
       "    print(self._show_string(n, truncate, vertical))\n",
       "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"C:\\Users\\SD-LORENZO-PC\\pyproject\\rndPy\\my_env\\Lib\\site-packages\\pyspark\\sql\\dataframe.py\", line 978, in _show_string\n",
       "    return self._jdf.showString(n, int_truncate, vertical)\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"C:\\Users\\SD-LORENZO-PC\\pyproject\\rndPy\\my_env\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n",
       "    return_value = get_return_value(\n",
       "                   ^^^^^^^^^^^^^^^^^\n",
       "  File \"C:\\Users\\SD-LORENZO-PC\\pyproject\\rndPy\\my_env\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 179, in deco\n",
       "    return f(*a, **kw)\n",
       "           ^^^^^^^^^^^\n",
       "  File \"C:\\Users\\SD-LORENZO-PC\\pyproject\\rndPy\\my_env\\Lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n",
       "    raise Py4JJavaError(\n",
       "py4j.protocol.Py4JJavaError: An error occurred while calling o85.showString.\n",
       ": org.apache.kafka.common.KafkaException: Failed to create new KafkaAdminClient\n",
       "\n",
       "\tat org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:540)\n",
       "\n",
       "\tat org.apache.kafka.clients.admin.Admin.create(Admin.java:144)\n",
       "\n",
       "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.createAdmin(ConsumerStrategy.scala:50)\n",
       "\n",
       "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.createAdmin$(ConsumerStrategy.scala:47)\n",
       "\n",
       "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.createAdmin(ConsumerStrategy.scala:102)\n",
       "\n",
       "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.admin(KafkaOffsetReaderAdmin.scala:70)\n",
       "\n",
       "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchPartitionOffsets(KafkaOffsetReaderAdmin.scala:128)\n",
       "\n",
       "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.getOffsetRangesFromUnresolvedOffsets(KafkaOffsetReaderAdmin.scala:374)\n",
       "\n",
       "\tat org.apache.spark.sql.kafka010.KafkaRelation.buildScan(KafkaRelation.scala:67)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSourceStrategy$.apply(DataSourceStrategy.scala:345)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n",
       "\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
       "\n",
       "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
       "\n",
       "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
       "\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\n",
       "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
       "\n",
       "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
       "\n",
       "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
       "\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
       "\n",
       "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
       "\n",
       "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
       "\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\n",
       "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
       "\n",
       "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
       "\n",
       "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
       "\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:476)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:162)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)\n",
       "\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:162)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:155)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:175)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)\n",
       "\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:175)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:168)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:221)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:266)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:235)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:112)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
       "\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
       "\n",
       "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4204)\n",
       "\n",
       "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3200)\n",
       "\n",
       "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3421)\n",
       "\n",
       "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:283)\n",
       "\n",
       "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:322)\n",
       "\n",
       "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\n",
       "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
       "\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
       "\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
       "\n",
       "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
       "\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
       "\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
       "\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
       "\n",
       "Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers\n",
       "\n",
       "\tat org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:89)\n",
       "\n",
       "\tat org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:48)\n",
       "\n",
       "\tat org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:494)\n",
       "\n",
       "\t... 87 more\n",
       "\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import regexp_replace, col, from_json\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "kafka_json_df = kafka_df.withColumn(\"value\", expr(\"cast(value as string)\"))\n",
    "kafka_json_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"_id\", StructType([\n",
    "        StructField(\"$oid\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"operationType\", StringType(), True),\n",
    "    StructField(\"clusterTime\", StructType([\n",
    "        StructField(\"$timestamp\", StructType([\n",
    "            StructField(\"t\", DoubleType(), True),\n",
    "            StructField(\"i\", DoubleType(), True)\n",
    "        ]), True)\n",
    "    ]), True),\n",
    "    StructField(\"wallTime\", StructType([\n",
    "        StructField(\"$date\", DoubleType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"fullDocument\", StructType([\n",
    "        StructField(\"_id\", StructType([\n",
    "            StructField(\"$oid\", StringType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"lat\", StringType(), True),\n",
    "        StructField(\"lon\", StringType(), True),\n",
    "        StructField(\"z\", StringType(), True),\n",
    "        StructField(\"time\", StringType(), True),\n",
    "        StructField(\"anom\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"ns\", StructType([\n",
    "        StructField(\"db\", StringType(), True),\n",
    "        StructField(\"coll\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"documentKey\", StructType([\n",
    "        StructField(\"_id\", StructType([\n",
    "            StructField(\"$oid\", StringType(), True)\n",
    "        ]), True)\n",
    "    ]), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# Remove unwanted characters using a stricter regular expression\n",
    "cleaned_kafka_df = kafka_json_df.withColumn(\n",
    "    \"clean_value\",\n",
    "    regexp_replace(col(\"value\"), r\"[^ -~]\", \"\",)  # Matches printable ASCII characters only\n",
    ")\n",
    "\n",
    "# Show the cleaned data\n",
    "# cleaned_kafka_df.select(\"clean_value\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json,col\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "streaming_df = cleaned_kafka_df.withColumn(\n",
    "    \"values_json\", \n",
    "    from_json(col(\"clean_value\"), schema)\n",
    ").selectExpr(\"values_json.*\")\n",
    "\n",
    "# Show the parsed DataFrame\n",
    "# streaming_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "root\n",
       " |-- _id: struct (nullable = true)\n",
       " |    |-- $oid: string (nullable = true)\n",
       " |-- operationType: string (nullable = true)\n",
       " |-- clusterTime: struct (nullable = true)\n",
       " |    |-- $timestamp: struct (nullable = true)\n",
       " |    |    |-- t: double (nullable = true)\n",
       " |    |    |-- i: double (nullable = true)\n",
       " |-- wallTime: struct (nullable = true)\n",
       " |    |-- $date: double (nullable = true)\n",
       " |-- fullDocument: struct (nullable = true)\n",
       " |    |-- _id: struct (nullable = true)\n",
       " |    |    |-- $oid: string (nullable = true)\n",
       " |    |-- lat: string (nullable = true)\n",
       " |    |-- lon: string (nullable = true)\n",
       " |    |-- z: string (nullable = true)\n",
       " |    |-- time: string (nullable = true)\n",
       " |    |-- anom: string (nullable = true)\n",
       " |-- ns: struct (nullable = true)\n",
       " |    |-- db: string (nullable = true)\n",
       " |    |-- coll: string (nullable = true)\n",
       " |-- documentKey: struct (nullable = true)\n",
       " |    |-- _id: struct (nullable = true)\n",
       " |    |    |-- $oid: string (nullable = true)\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "streaming_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "exploded_df = streaming_df.selectExpr(\"fullDocument\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "root\n",
       " |-- fullDocument: struct (nullable = true)\n",
       " |    |-- _id: struct (nullable = true)\n",
       " |    |    |-- $oid: string (nullable = true)\n",
       " |    |-- lat: string (nullable = true)\n",
       " |    |-- lon: string (nullable = true)\n",
       " |    |-- z: string (nullable = true)\n",
       " |    |-- time: string (nullable = true)\n",
       " |    |-- anom: string (nullable = true)\n",
       "\n",
       "Traceback (most recent call last):\n",
       "  File \"c:\\Users\\SD-LORENZO-PC\\.vscode\\extensions\\ms-python.python-2024.22.0-win32-x64\\python_files\\python_server.py\", line 133, in exec_user_input\n",
       "    retval = callable_(user_input, user_globals)\n",
       "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"<string>\", line 2, in <module>\n",
       "  File \"C:\\Users\\SD-LORENZO-PC\\pyproject\\rndPy\\my_env\\Lib\\site-packages\\pyspark\\sql\\dataframe.py\", line 947, in show\n",
       "    print(self._show_string(n, truncate, vertical))\n",
       "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"C:\\Users\\SD-LORENZO-PC\\pyproject\\rndPy\\my_env\\Lib\\site-packages\\pyspark\\sql\\dataframe.py\", line 978, in _show_string\n",
       "    return self._jdf.showString(n, int_truncate, vertical)\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"C:\\Users\\SD-LORENZO-PC\\pyproject\\rndPy\\my_env\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n",
       "    return_value = get_return_value(\n",
       "                   ^^^^^^^^^^^^^^^^^\n",
       "  File \"C:\\Users\\SD-LORENZO-PC\\pyproject\\rndPy\\my_env\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 179, in deco\n",
       "    return f(*a, **kw)\n",
       "           ^^^^^^^^^^^\n",
       "  File \"C:\\Users\\SD-LORENZO-PC\\pyproject\\rndPy\\my_env\\Lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n",
       "    raise Py4JJavaError(\n",
       "py4j.protocol.Py4JJavaError: An error occurred while calling o103.showString.\n",
       ": org.apache.kafka.common.KafkaException: Failed to create new KafkaAdminClient\n",
       "\n",
       "\tat org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:540)\n",
       "\n",
       "\tat org.apache.kafka.clients.admin.Admin.create(Admin.java:144)\n",
       "\n",
       "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.createAdmin(ConsumerStrategy.scala:50)\n",
       "\n",
       "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.createAdmin$(ConsumerStrategy.scala:47)\n",
       "\n",
       "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.createAdmin(ConsumerStrategy.scala:102)\n",
       "\n",
       "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.admin(KafkaOffsetReaderAdmin.scala:70)\n",
       "\n",
       "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchPartitionOffsets(KafkaOffsetReaderAdmin.scala:128)\n",
       "\n",
       "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.getOffsetRangesFromUnresolvedOffsets(KafkaOffsetReaderAdmin.scala:374)\n",
       "\n",
       "\tat org.apache.spark.sql.kafka010.KafkaRelation.buildScan(KafkaRelation.scala:67)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSourceStrategy$.apply(DataSourceStrategy.scala:345)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n",
       "\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
       "\n",
       "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
       "\n",
       "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
       "\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\n",
       "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
       "\n",
       "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
       "\n",
       "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
       "\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
       "\n",
       "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
       "\n",
       "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
       "\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\n",
       "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
       "\n",
       "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
       "\n",
       "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
       "\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:476)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:162)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)\n",
       "\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:162)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:155)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:175)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)\n",
       "\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:175)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:168)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:221)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:266)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:235)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:112)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
       "\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
       "\n",
       "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4204)\n",
       "\n",
       "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3200)\n",
       "\n",
       "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3421)\n",
       "\n",
       "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:283)\n",
       "\n",
       "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:322)\n",
       "\n",
       "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\n",
       "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
       "\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
       "\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
       "\n",
       "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
       "\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
       "\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
       "\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
       "\n",
       "Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers\n",
       "\n",
       "\tat org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:89)\n",
       "\n",
       "\tat org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:48)\n",
       "\n",
       "\tat org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:494)\n",
       "\n",
       "\t... 87 more\n",
       "\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exploded_df.printSchema()\n",
    "exploded_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Flatten the `fullDocument` struct\n",
    "flattened_fullDocument_df = streaming_df.select(\n",
    "    col(\"fullDocument._id.$oid\").alias(\"document_id\"),\n",
    "    col(\"fullDocument.lat\").alias(\"latitude\"),\n",
    "    col(\"fullDocument.lon\").alias(\"longitude\"),\n",
    "    col(\"fullDocument.z\").alias(\"altitude\"),\n",
    "    col(\"fullDocument.time\").alias(\"timestamp\"),\n",
    "    col(\"fullDocument.anom\").alias(\"anomaly\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "root\n",
       " |-- document_id: string (nullable = true)\n",
       " |-- latitude: string (nullable = true)\n",
       " |-- longitude: string (nullable = true)\n",
       " |-- altitude: string (nullable = true)\n",
       " |-- timestamp: string (nullable = true)\n",
       " |-- anomaly: string (nullable = true)\n",
       "\n",
       "Traceback (most recent call last):\n",
       "  File \"c:\\Users\\SD-LORENZO-PC\\.vscode\\extensions\\ms-python.python-2024.22.0-win32-x64\\python_files\\python_server.py\", line 133, in exec_user_input\n",
       "    retval = callable_(user_input, user_globals)\n",
       "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"<string>\", line 3, in <module>\n",
       "  File \"C:\\Users\\SD-LORENZO-PC\\pyproject\\rndPy\\my_env\\Lib\\site-packages\\pyspark\\sql\\dataframe.py\", line 947, in show\n",
       "    print(self._show_string(n, truncate, vertical))\n",
       "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"C:\\Users\\SD-LORENZO-PC\\pyproject\\rndPy\\my_env\\Lib\\site-packages\\pyspark\\sql\\dataframe.py\", line 978, in _show_string\n",
       "    return self._jdf.showString(n, int_truncate, vertical)\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"C:\\Users\\SD-LORENZO-PC\\pyproject\\rndPy\\my_env\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n",
       "    return_value = get_return_value(\n",
       "                   ^^^^^^^^^^^^^^^^^\n",
       "  File \"C:\\Users\\SD-LORENZO-PC\\pyproject\\rndPy\\my_env\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 179, in deco\n",
       "    return f(*a, **kw)\n",
       "           ^^^^^^^^^^^\n",
       "  File \"C:\\Users\\SD-LORENZO-PC\\pyproject\\rndPy\\my_env\\Lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n",
       "    raise Py4JJavaError(\n",
       "py4j.protocol.Py4JJavaError: An error occurred while calling o121.showString.\n",
       ": org.apache.kafka.common.KafkaException: Failed to create new KafkaAdminClient\n",
       "\n",
       "\tat org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:540)\n",
       "\n",
       "\tat org.apache.kafka.clients.admin.Admin.create(Admin.java:144)\n",
       "\n",
       "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.createAdmin(ConsumerStrategy.scala:50)\n",
       "\n",
       "\tat org.apache.spark.sql.kafka010.ConsumerStrategy.createAdmin$(ConsumerStrategy.scala:47)\n",
       "\n",
       "\tat org.apache.spark.sql.kafka010.SubscribeStrategy.createAdmin(ConsumerStrategy.scala:102)\n",
       "\n",
       "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.admin(KafkaOffsetReaderAdmin.scala:70)\n",
       "\n",
       "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.fetchPartitionOffsets(KafkaOffsetReaderAdmin.scala:128)\n",
       "\n",
       "\tat org.apache.spark.sql.kafka010.KafkaOffsetReaderAdmin.getOffsetRangesFromUnresolvedOffsets(KafkaOffsetReaderAdmin.scala:374)\n",
       "\n",
       "\tat org.apache.spark.sql.kafka010.KafkaRelation.buildScan(KafkaRelation.scala:67)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSourceStrategy$.apply(DataSourceStrategy.scala:345)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n",
       "\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
       "\n",
       "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
       "\n",
       "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
       "\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\n",
       "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
       "\n",
       "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
       "\n",
       "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
       "\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
       "\n",
       "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
       "\n",
       "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
       "\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\n",
       "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
       "\n",
       "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
       "\n",
       "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
       "\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n",
       "\n",
       "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n",
       "\n",
       "\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n",
       "\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\n",
       "\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n",
       "\n",
       "\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n",
       "\n",
       "\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n",
       "\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:476)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:162)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)\n",
       "\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:162)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:155)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:175)\n",
       "\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:202)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:202)\n",
       "\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:201)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:175)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:168)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:221)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:266)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:235)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:112)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
       "\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
       "\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
       "\n",
       "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4204)\n",
       "\n",
       "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3200)\n",
       "\n",
       "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3421)\n",
       "\n",
       "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:283)\n",
       "\n",
       "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:322)\n",
       "\n",
       "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\n",
       "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
       "\n",
       "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\n",
       "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
       "\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
       "\n",
       "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
       "\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
       "\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
       "\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
       "\n",
       "Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers\n",
       "\n",
       "\tat org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:89)\n",
       "\n",
       "\tat org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:48)\n",
       "\n",
       "\tat org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:494)\n",
       "\n",
       "\t... 101 more\n",
       "\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the schema of the flattened_df, place a sample json file and change readStream to read \n",
    "flattened_fullDocument_df.printSchema()\n",
    "flattened_fullDocument_df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
